{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfdaa5d-896b-48cd-89b9-f65efbdea7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os\n",
    "\n",
    "def check_gpu_memory(gpu_id):\n",
    "    \"\"\"æ£€æŸ¥æŒ‡å®šGPUçš„æ˜¾å­˜ä½¿ç”¨æƒ…å†µ\"\"\"\n",
    "    if torch.cuda.is_available() and gpu_id < torch.cuda.device_count():\n",
    "        try:\n",
    "            total_memory = torch.cuda.get_device_properties(gpu_id).total_memory / 1024**3\n",
    "            allocated = torch.cuda.memory_allocated(gpu_id) / 1024**3\n",
    "            cached = torch.cuda.memory_reserved(gpu_id) / 1024**3\n",
    "            free = total_memory - cached\n",
    "            \n",
    "            print(f\"GPU {gpu_id}:\")\n",
    "            print(f\"  æ€»æ˜¾å­˜: {total_memory:.1f}GB\")\n",
    "            print(f\"  å·²åˆ†é…: {allocated:.1f}GB\") \n",
    "            print(f\"  å·²ç¼“å­˜: {cached:.1f}GB\")\n",
    "            print(f\"  å¯ç”¨æ˜¾å­˜: {free:.1f}GB\")\n",
    "            \n",
    "            return allocated, cached, free\n",
    "        except Exception as e:\n",
    "            print(f\"GPU {gpu_id} æ£€æŸ¥å¤±è´¥: {e}\")\n",
    "            return 0, 0, 0\n",
    "    else:\n",
    "        print(f\"GPU {gpu_id} ä¸å¯ç”¨\")\n",
    "        return 0, 0, 0\n",
    "\n",
    "def clear_specific_gpu_memory(gpu_ids):\n",
    "    \"\"\"æ¸…ç†æŒ‡å®šGPUçš„æ˜¾å­˜\"\"\"\n",
    "    print(\"=== æ¸…ç†å‰çš„æ˜¾å­˜çŠ¶æ€ ===\")\n",
    "    for gpu_id in gpu_ids:\n",
    "        check_gpu_memory(gpu_id)\n",
    "    \n",
    "    print(f\"\\nå¼€å§‹æ¸…ç†GPU {gpu_ids}çš„æ˜¾å­˜...\")\n",
    "    \n",
    "    for gpu_id in gpu_ids:\n",
    "        try:\n",
    "            # è®¾ç½®å½“å‰è®¾å¤‡\n",
    "            torch.cuda.set_device(gpu_id)\n",
    "            \n",
    "            # æ¸…ç†ç¼“å­˜\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # å¼ºåˆ¶åƒåœ¾å›æ”¶\n",
    "            gc.collect()\n",
    "            \n",
    "            print(f\"âœ… GPU {gpu_id} æ˜¾å­˜æ¸…ç†å®Œæˆ\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ GPU {gpu_id} æ¸…ç†å¤±è´¥: {e}\")\n",
    "    \n",
    "    print(\"\\n=== æ¸…ç†åçš„æ˜¾å­˜çŠ¶æ€ ===\")\n",
    "    total_free = 0\n",
    "    for gpu_id in gpu_ids:\n",
    "        allocated, cached, free = check_gpu_memory(gpu_id)\n",
    "        total_free += free\n",
    "    \n",
    "    print(f\"\\nGPU {gpu_ids} æ€»å¯ç”¨æ˜¾å­˜: {total_free:.1f}GB\")\n",
    "    return total_free\n",
    "\n",
    "def clear_pytorch_cache():\n",
    "    \"\"\"æ¸…ç†PyTorchç¼“å­˜\"\"\"\n",
    "    print(\"\\næ¸…ç†PyTorchå…¨å±€ç¼“å­˜...\")\n",
    "    if hasattr(torch.cuda, 'empty_cache'):\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"âœ… PyTorchç¼“å­˜æ¸…ç†å®Œæˆ\")\n",
    "\n",
    "# ä¸»ç¨‹åº\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"GPUæ˜¾å­˜æ¸…ç†å·¥å…·\")\n",
    "    print(\"ç›®æ ‡: åªæ¸…ç†GPU 0å’ŒGPU 2ï¼Œä¿æŒGPU 1å’ŒGPU 3ä¸å˜\")\n",
    "    \n",
    "    # æ£€æŸ¥CUDAå¯ç”¨æ€§\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"âŒ CUDAä¸å¯ç”¨ï¼Œæ— æ³•æ¸…ç†GPUæ˜¾å­˜\")\n",
    "        exit()\n",
    "    \n",
    "    print(f\"æ£€æµ‹åˆ° {torch.cuda.device_count()} ä¸ªGPU\")\n",
    "    \n",
    "    # è¦æ¸…ç†çš„GPUåˆ—è¡¨\n",
    "    target_gpus = [0, 2]\n",
    "    \n",
    "    # æ˜¾ç¤ºæ‰€æœ‰GPUçŠ¶æ€ï¼ˆç”¨äºå¯¹æ¯”ï¼‰\n",
    "    print(\"\\n=== æ‰€æœ‰GPUå½“å‰çŠ¶æ€ ===\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        check_gpu_memory(i)\n",
    "    \n",
    "    # æ¸…ç†æŒ‡å®šGPU\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    available_memory = clear_specific_gpu_memory(target_gpus)\n",
    "    \n",
    "    # é¢å¤–æ¸…ç†PyTorchç¼“å­˜\n",
    "    clear_pytorch_cache()\n",
    "    \n",
    "    # æœ€ç»ˆæ£€æŸ¥\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"=== æœ€ç»ˆçŠ¶æ€æ£€æŸ¥ ===\")\n",
    "    \n",
    "    final_free = 0\n",
    "    for gpu_id in target_gpus:\n",
    "        allocated, cached, free = check_gpu_memory(gpu_id)\n",
    "        final_free += free\n",
    "    \n",
    "    print(f\"\\nğŸ¯ GPU 0å’Œ2æ€»å¯ç”¨æ˜¾å­˜: {final_free:.1f}GB\")\n",
    "    \n",
    "    # è¯„ä¼°æ˜¯å¦è¶³å¤ŸåŠ è½½æ¨¡å‹\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"=== æ¨¡å‹åŠ è½½å»ºè®® ===\")\n",
    "    \n",
    "    if final_free >= 17:\n",
    "        print(\"âœ… æ˜¾å­˜å……è¶³ï¼Œå¯ä»¥å°è¯•INT4é‡åŒ–åŠ è½½Qwen2.5-14B\")\n",
    "        print(\"å»ºè®®ä»£ç :\")\n",
    "        print(\"\"\"\n",
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,2\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/home/dataset-assist-0/user/wangshuo/HYR/0628-LLMä»»åŠ¡/1.Qwen2.5-14B-Instruct\",\n",
    "    local_files_only=True,\n",
    "    trust_remote_code=True,\n",
    "    load_in_4bit=True,  # 4ä½é‡åŒ–\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "        \"\"\")\n",
    "    \n",
    "    print(f\"\\næ¸…ç†å®Œæˆï¼GPU 0å’Œ2ç°åœ¨æœ‰ {final_free:.1f}GB å¯ç”¨æ˜¾å­˜ã€‚\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
